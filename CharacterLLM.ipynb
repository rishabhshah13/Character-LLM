{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install --upgrade openai\n",
    "# !pip install openai==0.28"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python run_api_gen_data.py --prompt_name gen_scene --character CassiusDio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI(\n",
    "    base_url = 'http://localhost:11434/v1',\n",
    "    api_key='ollama', # required, but unused\n",
    ")\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "  model=\"gemma\",\n",
    "  messages=[\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "    # {\"role\": \"user\", \"content\": \"Who won the world series in 2020?\"},\n",
    "    # {\"role\": \"assistant\", \"content\": \"The LA Dodgers won in 2020.\"},\n",
    "    # {\"role\": \"user\", \"content\": \"Where was it played?\"}\n",
    "  ]\n",
    ")\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ollama\n",
    "# client = OpenAI(\n",
    "# base_url = 'http://localhost:11434/v1',\n",
    "# api_key='ollama', # required, but unused\n",
    "# )\n",
    "# response = client.chat.completions.create(\n",
    "#     model=\"gemma\",\n",
    "#     messages=[\n",
    "#         {\"role\": \"system\", \"content\": sys_prompt_content},\n",
    "#         {\"role\": \"user\", \"content\": input},\n",
    "#         ],\n",
    "#     max_tokens=max_tokens,\n",
    "#     temperature=temperature,\n",
    "#     top_p=top_p,\n",
    "#     frequency_penalty=frequency_penalty,\n",
    "#     presence_penalty=presence_penalty,\n",
    "#     n=n,\n",
    "#     stop=stop,\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from groq import Groq\n",
    "\n",
    "client = Groq(\n",
    "    api_key='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',\n",
    ")\n",
    "\n",
    "completion = client.chat.completions.create(\n",
    "    model=\"mixtral-8x7b-32768\",\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"\"\n",
    "        }\n",
    "    ],\n",
    "    temperature=0.5,\n",
    "    max_tokens=1024,\n",
    "    top_p=1,\n",
    "    stream=True,\n",
    "    stop=None,\n",
    ")\n",
    "\n",
    "for chunk in completion:\n",
    "    print(chunk.choices[0].delta.content or \"\", end=\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !python parser/parse_data_scene.py result/2023-10-08/gen_scene/gpt-3.5-turbo-temp-0.7-char-CassiusDio.jsonl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !python run_api_gen_data.py --prompt_name gen_dialogue --character CassiusDio --data_path processed/2023-10-08/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !python parser/parse_data_dialogue.py result/2023-10-08/gen_dialogue/gpt-3.5-turbo-temp-0.7-char-CassiusDio.jsonl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# python run_api_gen_data.py --prompt_name gen_hallucination --character CassiusDio --data_path processed/2023-10-08/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# python parser/parse_data_hallucination.py result/2023-10-08/gen_hallucination/gpt-3.5-turbo-temp-0.7-char-CassiusDio.jsonl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "python parser/convert_prompt_data.py processed/2023-10-08/generated_agent_dialogue_CassiusDio.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# conda install pytorch==2.1.2 torchvision==0.16.2 torchaudio==2.1.2 pytorch-cuda=12.1 -c pytorch -c nvidia\n",
    "# conda install cuda -c nvidia\n",
    "# pip3 install \"fschat[model_worker,webui]\"\n",
    "# git clone https://github.com/lm-sys/FastChat.git\n",
    "# cd FastChat\n",
    "# pip3 install -e \".[train]\"\n",
    "\n",
    "# cd /mnt/c/Users/rs659/Desktop/Pliny/Character-LLM\n",
    "# conda activate vllmenv\n",
    "\n",
    "\n",
    "\n",
    "# ENVARS ADDED **ONLY FOR READABILITY**\n",
    "NVIDIA_CUDA_PPA=https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2004/x86_64/\n",
    "NVIDIA_CUDA_PREFERENCES=https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2004/x86_64/cuda-ubuntu2004.pin\n",
    "NVIDIA_CUDA_PUBKEY=https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2004/x86_64/7fa2af80.pub\n",
    "\n",
    "# Add NVIDIA Developers 3rd-Party PPA\n",
    "sudo wget ${NVIDIA_CUDA_PREFERENCES} -O /etc/apt/preferences.d/nvidia-cuda\n",
    "sudo apt-key adv --fetch-keys ${NVIDIA_CUDA_PUBKEY}\n",
    "echo \"deb ${NVIDIA_CUDA_PPA} /\" | sudo tee /etc/apt/sources.list.d/nvidia-cuda.list\n",
    "\n",
    "# Install development tools\n",
    "sudo apt update\n",
    "sudo apt install -y cuda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "train_json = json.load(open('processed\\\\2023-10-08\\\\prompted\\\\prompted_agent_dialogue_CassiusDio.jsonl', \"r\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cd C:\\Users\\rs659\\Desktop\\Pliny\\Character-LLM\n",
    "\n",
    "\n",
    "# Scene Extraction\n",
    "# python run_api_gen_data.py --prompt_name gen_scene --character Beethoven --debug 1\n",
    "# python parser/parse_data_scene.py result/2023-10-08/gen_scene/gpt-3.5-turbo-temp-0.7-char-Beethoven.jsonl\n",
    "\n",
    "# Experience Completion\n",
    "# python run_api_gen_data.py --prompt_name gen_dialogue --character Beethoven --data_path processed/2023-10-08/ --debug 1\n",
    "# python parser/parse_data_dialogue.py result/2023-10-08/gen_dialogue/gpt-3.5-turbo-temp-0.7-char-Beethoven.jsonl\n",
    "\n",
    "# Protective Scene\n",
    "# python run_api_gen_data.py --prompt_name gen_hallucination --character Beethoven --data_path processed/2023-10-08/ --debug 1\n",
    "# python parser/parse_data_hallucination.py result/2023-10-08/gen_hallucination/gpt-3.5-turbo-temp-0.7-char-Beethoven.jsonl\n",
    "\n",
    "\n",
    "# Convert to Training Format\n",
    "# python parser/convert_prompt_data.py processed/2023-10-08/generated_agent_dialogue_Beethoven.json\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cd FastChat\n",
    "# export CHARACTER=CassiusDio\n",
    "\n",
    "\n",
    "torchrun --nproc_per_node=1 --master_port=20031 fastchat/train/train_mem.py --model_name_or_path unsloth/gemma-2b --data_path /mnt/c/Users/rs659/Desktop/Pliny/Character-LLM/processed/2023-10-08/prompted/prompted_agent_dialogue_Beethoven.jsonl  --output_dir /mnt/c/Users/rs659/Desktop/Pliny/Character-LLM/ckpt/Beethoven_7b --num_train_epochs 10\n",
    "\n",
    "torchrun --nproc_per_node=1 --master_port=20031 fastchat/train/train_mem.py --model_name_or_path unsloth/gemma-2b --data_path ../processed/2023-10-08/prompted/prompted_agent_dialogue_CassiusDio.jsonl  --output_dir ../ckpt/${CHARACTER}_7b --num_train_epochs 10\n",
    "\n",
    "torchrun --nproc_per_node=1 --master_port=20031 fastchat/train/train_mem.py --model_name_or_path winglian/Llama-2-3b-hf --data_path /mnt/c/Users/rs659/Desktop/Pliny/Character-LLM/processed/2023-10-08/prompted/prompted_agent_dialogue_Beethoven.jsonl   --already_preprocess True --bf16 False --output_dir /mnt/c/Users/rs659/Desktop/Pliny/Character-LLM/ckpt/Beethoven_7b --num_train_epochs 1 --per_device_train_batch_size 1 --per_device_eval_batch_size 1 --gradient_accumulation_steps 4 --evaluation_strategy epoch --save_strategy epoch --save_total_limit 10 --learning_rate 2e-5 --weight_decay 0.1 --warmup_ratio 0.04 --lr_scheduler_type cosine --logging_steps 1 --fsdp 'full_shard auto_wrap' --fsdp_transformer_layer_cls_to_wrap LlamaDecoderLayer --tf32 False --model_max_length 512 --gradient_checkpointing True\n",
    "\n",
    "# Working\n",
    "torchrun --nproc_per_node=1 --master_port=20031 fastchat/train/train_mem.py --model_name_or_path winglian/Llama-2-3b-hf --data_path /mnt/c/Users/rs659/Desktop/Pliny/Character-LLM/processed/2023-10-08/prompted/prompted_agent_dialogue_Beethoven.jsonl   --already_preprocess True --bf16 True --output_dir /mnt/c/Users/rs659/Desktop/Pliny/Character-LLM/ckpt/Beethoven_7b --num_train_epochs 1 --per_device_train_batch_size 1 --per_device_eval_batch_size 1 --gradient_accumulation_steps 4 --evaluation_strategy epoch --save_strategy epoch --save_total_limit 10 --learning_rate 2e-5 --weight_decay 0.1 --warmup_ratio 0.04 --lr_scheduler_type cosine --logging_steps 1 --fsdp 'full_shard auto_wrap' --fsdp_transformer_layer_cls_to_wrap LlamaDecoderLayer --tf32 True --model_max_length 2048 --gradient_checkpointing True\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "!torchrun --nproc_per_node=1 --master_port=20031 fastchat/train/train_mem.py --model_name_or_path meta-llama/Llama-2-7b-hf --data_path /path/to/prompted_agent_dialogue_$CHARACTER.json --already_preprocess True --bf16 True --output_dir /path/to/ckpt/${CHARACTER}_7b --num_train_epochs 10 --per_device_train_batch_size 1 --per_device_eval_batch_size 16 --gradient_accumulation_steps 4 --evaluation_strategy epoch --save_strategy epoch --save_total_limit 10 --learning_rate 2e-5 --weight_decay 0.1 --warmup_ratio 0.04 --lr_scheduler_type cosine --logging_steps 1 --fsdp 'full_shard auto_wrap' --fsdp_transformer_layer_cls_to_wrap LlamaDecoderLayer --tf32 True --model_max_length 2048 --gradient_checkpointing True --num-gpus 1\n",
    "\n",
    "\n",
    "deepspeed fastchat/train/train_lora.py\n",
    "--model_name_or_path lmsys/vicuna-7b-v1.5\n",
    "--lora_r 2\n",
    "--lora_alpha 16\n",
    "--lora_dropout 0.05\n",
    "--data_path /home/n/FastChat/data/dummy_conversation.json\n",
    "--bf16 True\n",
    "--output_dir ./checkpoints\n",
    "--num_train_epochs 3\n",
    "--per_device_train_batch_size 1\n",
    "--per_device_eval_batch_size 1\n",
    "--gradient_accumulation_steps 1\n",
    "--evaluation_strategy \"no\"\n",
    "--save_strategy \"steps\"\n",
    "--save_steps 1200\n",
    "--save_total_limit 100\n",
    "--learning_rate 2e-5\n",
    "--weight_decay 0.\n",
    "--warmup_ratio 0.03\n",
    "--lr_scheduler_type \"cosine\"\n",
    "--logging_steps 1\n",
    "--tf32 True\n",
    "--model_max_length 2048\n",
    "--q_lora True\n",
    "--deepspeed /home/n/FastChat/playground/deepspeed_config_s2.json`\n",
    "\n",
    "\n",
    "\n",
    "torchrun --nproc_per_node=4 --master_port=20001 fastchat/train/train_mem.py \\\n",
    "    --model_name_or_path meta-llama/Llama-2-7b-hf \\\n",
    "    --data_path data/dummy_conversation.json \\\n",
    "    --bf16 True \\\n",
    "    --output_dir output_vicuna \\\n",
    "    --num_train_epochs 3 \\\n",
    "    --per_device_train_batch_size 2 \\\n",
    "    --per_device_eval_batch_size 2 \\\n",
    "    --gradient_accumulation_steps 16 \\\n",
    "    --evaluation_strategy \"no\" \\\n",
    "    --save_strategy \"steps\" \\\n",
    "    --save_steps 1200 \\\n",
    "    --save_total_limit 10 \\\n",
    "    --learning_rate 2e-5 \\\n",
    "    --weight_decay 0. \\\n",
    "    --warmup_ratio 0.03 \\\n",
    "    --lr_scheduler_type \"cosine\" \\\n",
    "    --logging_steps 1 \\\n",
    "    --fsdp \"full_shard auto_wrap\" \\\n",
    "    --fsdp_transformer_layer_cls_to_wrap 'LlamaDecoderLayer' \\\n",
    "    --tf32 True \\\n",
    "    --model_max_length 2048 \\\n",
    "    --gradient_checkpointing True \\\n",
    "    --lazy_preprocess True\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# !torchrun --nproc_per_node=8 --master_port=20031 fastchat/train/train_mem.py \\\n",
    "#     --model_name_or_path /path/hf_model/llama-7b  \\\n",
    "#     --data_path /path/to/prompted_agent_dialogue_$CHARACTER.json \\\n",
    "#     --already_preprocess True \\\n",
    "#     --bf16 True \\\n",
    "#     --output_dir /path/to/ckpt/${CHARACTER}_7b \\\n",
    "#     --num_train_epochs 10 \\\n",
    "#     --per_device_train_batch_size 2 \\\n",
    "#     --per_device_eval_batch_size 16 \\\n",
    "#     --gradient_accumulation_steps 4 \\\n",
    "#     --evaluation_strategy epoch \\\n",
    "#     --save_strategy epoch \\\n",
    "#     --save_total_limit 10 \\\n",
    "#     --learning_rate 2e-5 \\\n",
    "#     --weight_decay 0.1 \\\n",
    "#     --warmup_ratio 0.04 \\\n",
    "#     --lr_scheduler_type cosine \\\n",
    "#     --logging_steps 1 \\\n",
    "#     --fsdp 'full_shard auto_wrap' \\\n",
    "#     --fsdp_transformer_layer_cls_to_wrap LlamaDecoderLayer \\\n",
    "#     --tf32 True \\\n",
    "#     --model_max_length 2048 \\\n",
    "#     --gradient_checkpointing True \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # one WSL\n",
    "# # conda create -n vllmenv python=3.9\n",
    "# # conda activate vllmenv\n",
    "# # pip install transformers\n",
    "# # pip install torch==2.1.0 torchvision==0.16.0 torchaudio==2.1.0 --index-url https://download.pytorch.org/whl/cu121\n",
    "# # pip install git+https://github.com/casper-hansen/AutoAWQ.git\n",
    "# # pip install vllm\n",
    "# # pip install -U ipywidgets\n",
    "\n",
    "# conda activate vllmenv\n",
    "# python3 -m vllm.entrypoints.api_server --model TheBloke/Llama-2-7b-Chat-AWQ --quantization awq\n",
    "\n",
    "\n",
    "# # python -m vllm.entrypoints.openai.api_server --model TheBloke/Llama-2-7B-Chat-AWQ --api-key token-abc123"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from openai import OpenAI\n",
    "# client = OpenAI(\n",
    "#     base_url=\"http://localhost:8000/v1\",\n",
    "#     api_key=\"token-abc123\",\n",
    "# )\n",
    "\n",
    "# completion = client.chat.completions.create(\n",
    "#   model=\"mistralai/Mistral-7B-Instruct-v0.1\",\n",
    "#   messages=[\n",
    "#     {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "#     {\"role\": \"user\", \"content\": \"Hello!\"}\n",
    "#   ]\n",
    "# )\n",
    "\n",
    "# print(completion.choices[0].message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "plinyenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
