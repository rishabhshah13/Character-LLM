{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install --upgrade openai\n",
    "# !pip install openai==0.28"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python run_api_gen_data.py --prompt_name gen_scene --character CassiusDio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI(\n",
    "    base_url = 'http://localhost:11434/v1',\n",
    "    api_key='ollama', # required, but unused\n",
    ")\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "  model=\"gemma\",\n",
    "  messages=[\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "    # {\"role\": \"user\", \"content\": \"Who won the world series in 2020?\"},\n",
    "    # {\"role\": \"assistant\", \"content\": \"The LA Dodgers won in 2020.\"},\n",
    "    # {\"role\": \"user\", \"content\": \"Where was it played?\"}\n",
    "  ]\n",
    ")\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ollama\n",
    "# client = OpenAI(\n",
    "# base_url = 'http://localhost:11434/v1',\n",
    "# api_key='ollama', # required, but unused\n",
    "# )\n",
    "# response = client.chat.completions.create(\n",
    "#     model=\"gemma\",\n",
    "#     messages=[\n",
    "#         {\"role\": \"system\", \"content\": sys_prompt_content},\n",
    "#         {\"role\": \"user\", \"content\": input},\n",
    "#         ],\n",
    "#     max_tokens=max_tokens,\n",
    "#     temperature=temperature,\n",
    "#     top_p=top_p,\n",
    "#     frequency_penalty=frequency_penalty,\n",
    "#     presence_penalty=presence_penalty,\n",
    "#     n=n,\n",
    "#     stop=stop,\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from groq import Groq\n",
    "\n",
    "client = Groq(\n",
    "    api_key='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',\n",
    ")\n",
    "\n",
    "completion = client.chat.completions.create(\n",
    "    model=\"mixtral-8x7b-32768\",\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"\"\n",
    "        }\n",
    "    ],\n",
    "    temperature=0.5,\n",
    "    max_tokens=1024,\n",
    "    top_p=1,\n",
    "    stream=True,\n",
    "    stop=None,\n",
    ")\n",
    "\n",
    "for chunk in completion:\n",
    "    print(chunk.choices[0].delta.content or \"\", end=\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !python parser/parse_data_scene.py result/2023-10-08/gen_scene/gpt-3.5-turbo-temp-0.7-char-CassiusDio.jsonl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !python run_api_gen_data.py --prompt_name gen_dialogue --character CassiusDio --data_path processed/2023-10-08/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !python parser/parse_data_dialogue.py result/2023-10-08/gen_dialogue/gpt-3.5-turbo-temp-0.7-char-CassiusDio.jsonl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# python run_api_gen_data.py --prompt_name gen_hallucination --character CassiusDio --data_path processed/2023-10-08/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# python parser/parse_data_hallucination.py result/2023-10-08/gen_hallucination/gpt-3.5-turbo-temp-0.7-char-CassiusDio.jsonl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "python parser/convert_prompt_data.py processed/2023-10-08/generated_agent_dialogue_CassiusDio.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# conda install pytorch==2.1.2 torchvision==0.16.2 torchaudio==2.1.2 pytorch-cuda=12.1 -c pytorch -c nvidia\n",
    "# conda install cuda -c nvidia\n",
    "# pip3 install \"fschat[model_worker,webui]\"\n",
    "# git clone https://github.com/lm-sys/FastChat.git\n",
    "# cd FastChat\n",
    "# pip3 install -e \".[train]\"\n",
    "\n",
    "# cd /mnt/c/Users/rs659/Desktop/Pliny/Character-LLM\n",
    "# conda activate vllmenv\n",
    "\n",
    "\n",
    "\n",
    "# ENVARS ADDED **ONLY FOR READABILITY**\n",
    "NVIDIA_CUDA_PPA=https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2004/x86_64/\n",
    "NVIDIA_CUDA_PREFERENCES=https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2004/x86_64/cuda-ubuntu2004.pin\n",
    "NVIDIA_CUDA_PUBKEY=https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2004/x86_64/7fa2af80.pub\n",
    "\n",
    "# Add NVIDIA Developers 3rd-Party PPA\n",
    "sudo wget ${NVIDIA_CUDA_PREFERENCES} -O /etc/apt/preferences.d/nvidia-cuda\n",
    "sudo apt-key adv --fetch-keys ${NVIDIA_CUDA_PUBKEY}\n",
    "echo \"deb ${NVIDIA_CUDA_PPA} /\" | sudo tee /etc/apt/sources.list.d/nvidia-cuda.list\n",
    "\n",
    "# Install development tools\n",
    "sudo apt update\n",
    "sudo apt install -y cuda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "train_json = json.load(open('processed\\\\2023-10-08\\\\prompted\\\\prompted_agent_dialogue_CassiusDio.jsonl', \"r\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cd C:\\Users\\rs659\\Desktop\\Pliny\\Character-LLM\n",
    "\n",
    "\n",
    "# Scene Extraction\n",
    "# python run_api_gen_data.py --prompt_name gen_scene --character Beethoven --debug 1\n",
    "# python parser/parse_data_scene.py result/2023-10-08/gen_scene/gpt-3.5-turbo-temp-0.7-char-Beethoven.jsonl\n",
    "\n",
    "# Experience Completion\n",
    "# python run_api_gen_data.py --prompt_name gen_dialogue --character Beethoven --data_path processed/2023-10-08/ --debug 1\n",
    "# python parser/parse_data_dialogue.py result/2023-10-08/gen_dialogue/gpt-3.5-turbo-temp-0.7-char-Beethoven.jsonl\n",
    "\n",
    "# Protective Scene\n",
    "# python run_api_gen_data.py --prompt_name gen_hallucination --character Beethoven --data_path processed/2023-10-08/ --debug 1\n",
    "# python parser/parse_data_hallucination.py result/2023-10-08/gen_hallucination/gpt-3.5-turbo-temp-0.7-char-Beethoven.jsonl\n",
    "\n",
    "\n",
    "# Convert to Training Format\n",
    "# python parser/convert_prompt_data.py processed/2023-10-08/generated_agent_dialogue_Beethoven.json\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# conda create -n finetuneenv python=3.9\n",
    "# pip install torch==2.1.0 torchvision==0.16.0 torchaudio==2.1.0 --index-url https://download.pytorch.org/whl/cu121\n",
    "\n",
    "\n",
    "\n",
    "python finetune.py \\\n",
    "    --base_model 'decapoda-research/llama-7b-hf' \\\n",
    "    --data_path '/mnt/c/Users/rs659/Desktop/Pliny/Character-LLM/processed/2023-10-08/prompted/prompted_agent_dialogue_Beethoven.jsonl' \\\n",
    "    --output_dir './lora-alpaca'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset format Fastchat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ProcessedDataset(Dataset):\n",
    "    \"\"\"Dataset for supervised fine-tuning.\"\"\"\n",
    "\n",
    "    def __init__(self, raw_data, tokenizer: transformers.PreTrainedTokenizer):\n",
    "        super(ProcessedDataset, self).__init__()\n",
    "\n",
    "        rank0_print(\"Formatting inputs...\")\n",
    "        sources = [ex['prompt'] for ex in raw_data]\n",
    "        targets = [ex['output'] for ex in raw_data]\n",
    "        data_dict = alpaca_preprocess(sources, targets, tokenizer)\n",
    "\n",
    "        self.input_ids = data_dict[\"input_ids\"]\n",
    "        self.labels = data_dict[\"labels\"]\n",
    "        self.attention_mask = data_dict[\"attention_mask\"]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.input_ids)\n",
    "\n",
    "    def __getitem__(self, i) -> Dict[str, torch.Tensor]:\n",
    "        return dict(\n",
    "            input_ids=self.input_ids[i],\n",
    "            labels=self.labels[i],\n",
    "            attention_mask=self.attention_mask[i],\n",
    "        )\n",
    "\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "local_rank = None\n",
    "def rank0_print(*args):\n",
    "    if local_rank == 0:\n",
    "        print(*args)\n",
    "\n",
    "\n",
    "# data='/mnt/c/Users/rs659/Desktop/Pliny/Character-LLM/processed/2023-10-08/prompted/prompted_agent_dialogue_Beethoven.jsonl'\n",
    "data='C:/Users/rs659/Desktop/Pliny/Character-LLM/processed/2023-10-08/prompted/prompted_agent_dialogue_Beethoven.jsonl'\n",
    "raw_data =[]\n",
    "with open(data, 'r', encoding='utf-8') as fp:\n",
    "    for line in fp:\n",
    "        if line:\n",
    "            raw_data.append(json.loads(line))\n",
    "perm = np.random.permutation(len(raw_data))\n",
    "split = int(len(perm) * 0.98)                                                                                                               \n",
    "train_indices = perm[:split]\n",
    "eval_indices = perm[split:]\n",
    "train_raw_data = [raw_data[i] for i in train_indices]\n",
    "eval_raw_data = [raw_data[i] for i in eval_indices]\n",
    "rank0_print(f\"#train {len(train_raw_data)}, #eval {len(eval_raw_data)}\")\n",
    "\n",
    "# train_dataset = dataset_cls(train_raw_data, tokenizer=tokenizer)\n",
    "# eval_dataset = dataset_cls(eval_raw_data, tokenizer=tokenizer)\n",
    "# return dict(train_dataset=train_dataset, eval_dataset=eval_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom Dataset and training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://medium.com/@iamarunbrahma/fine-tuning-of-falcon-7b-large-language-model-using-qlora-on-mental-health-dataset-aa290eb6ec85\n",
    "\n",
    "# Windows Environent\n",
    "# conda create -n plinyenv python=3.9\n",
    "# conda activate plinyenv\n",
    "# pip install transformers\n",
    "# pip install torch==2.1.0 torchvision==0.16.0 torchaudio==2.1.0 --index-url https://download.pytorch.org/whl/cu121\n",
    "# pip install -U ipywidgets\n",
    "# pip install trl==0.6.0 transformers==4.32.0 accelerate>=0.20.3 peft==0.5.0 -Uqqq\n",
    "# pip install datasets==2.13.1 bitsandbytes==0.41.1 einops==0.7.0 wandb==0.15.8 -Uqqq\n",
    "# pip install gradio==3.41.0 transformers==4.32.0 langchain==0.0.273 -Uqqq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig, TrainingArguments, GenerationConfig\n",
    "from peft import LoraConfig, get_peft_model, PeftConfig, PeftModel, prepare_model_for_kbit_training\n",
    "from trl import SFTTrainer\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import numpy as np\n",
    "\n",
    "data_path = 'C:/Users/rs659/Desktop/Pliny/Character-LLM/processed/2023-10-08/prompted/prompted_agent_dialogue_Beethoven.jsonl'\n",
    "\n",
    "\n",
    "raw_data =[]\n",
    "with open(data_path, 'r', encoding='utf-8') as fp:\n",
    "    for line in fp:\n",
    "        if line:\n",
    "            raw_data.append(json.loads(line))\n",
    "perm = np.random.permutation(len(raw_data))\n",
    "split = int(len(perm) * 0.98)                                                                                                               \n",
    "train_indices = perm[:split]\n",
    "eval_indices = perm[split:]\n",
    "train_raw_data = [raw_data[i] for i in train_indices]\n",
    "eval_raw_data = [raw_data[i] for i in eval_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6NPV6477VHWUDJFTYPK33T34N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "small_data = [{'prompt': \"I wllows:\\n\\n\",\n",
    "  'output': 'Beethovennd power.<|eot|>',\n",
    "  'source': 'seed_dialogue_188'},\n",
    " {'prompt': \"I want you to act like Ludwig van Beethoven. I want you to respond and answer like Ludwig van Beethlows:\\n\\n\",\n",
    "  'output': \"Beethoven (thinking): This appointment as Kapellmeister is a culmination of my dedication to musl who partake in its wondrous embrace.<|eot|>\",\n",
    "  'source': 'seed_dialogue_335'}]\n",
    "\n",
    "\n",
    "# formatted_data = [{\"text\": small_data[i][\"prompt\"], \"output\": small_data[i][\"output\"]} for i in range(len(small_data))]\n",
    "formatted_data = [(\"<HUMAN>:\" + train_raw_data[i][\"prompt\"] + \" <ASSISTANT>: \" + train_raw_data[i][\"output\"]) for i in range(len(train_raw_data))]\n",
    "\n",
    "df = pd.DataFrame({'text': formatted_data})\n",
    "\n",
    "df.to_csv('formatted_data.csv',index=False)\n",
    "\n",
    "from datasets import load_dataset\n",
    "dataset = load_dataset('csv', data_files='formatted_data.csv')\n",
    "\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = load_dataset(\"heliosbrahma/mental_health_chatbot_dataset\")\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from datasets import load_dataset\n",
    "# dataset = load_dataset('json', data_files=data_path)\n",
    "\n",
    "# dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"ybelkada/falcon-7b-sharded-bf16\" # sharded falcon-7b model\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,            # load model in 4-bit precision\n",
    "    bnb_4bit_quant_type=\"nf4\",    # pre-trained model should be quantized in 4-bit NF format\n",
    "    bnb_4bit_use_double_quant=True, # Using double quantization as mentioned in QLoRA paper\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16, # During computation, pre-trained model should be loaded in BF16 format\n",
    ")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    quantization_config=bnb_config, # Use bitsandbytes config\n",
    "    device_map=\"auto\",  # Specifying device_map=\"auto\" so that HF Accelerate will determine which GPU to put each layer of the model on\n",
    "    trust_remote_code=True, # Set trust_remote_code=True to use falcon-7b model with custom code\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "tokenizer.pad_token = tokenizer.eos_token # Setting pad_token same as eos_token\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = prepare_model_for_kbit_training(model)\n",
    "\n",
    "lora_alpha = 32 # scaling factor for the weight matrices\n",
    "lora_dropout = 0.05 # dropout probability of the LoRA layers\n",
    "lora_rank = 32 # dimension of the low-rank matrices\n",
    "\n",
    "peft_config = LoraConfig(\n",
    "    lora_alpha=lora_alpha,\n",
    "    lora_dropout=lora_dropout,\n",
    "    r=lora_rank,\n",
    "    bias=\"none\",  # setting to 'none' for only training weight params instead of biases\n",
    "    task_type=\"CAUSAL_LM\",\n",
    "    target_modules=[         # Setting names of modules in falcon-7b model that we want to apply LoRA to\n",
    "        \"query_key_value\",\n",
    "        \"dense\",\n",
    "        \"dense_h_to_4h\",\n",
    "        \"dense_4h_to_h\",\n",
    "    ]\n",
    ")\n",
    "\n",
    "peft_model = get_peft_model(model, peft_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = \"./falcon-7b-sharded-bf16-finetuned-mental-health-conversational\"\n",
    "per_device_train_batch_size = 2 # reduce batch size by 2x if out-of-memory error\n",
    "gradient_accumulation_steps = 6  # increase gradient accumulation steps by 2x if batch size is reduced\n",
    "optim = \"paged_adamw_32bit\" # activates the paging for better memory management\n",
    "save_strategy=\"steps\" # checkpoint save strategy to adopt during training\n",
    "save_steps = 10 # number of updates steps before two checkpoint saves\n",
    "logging_steps = 10  # number of update steps between two logs if logging_strategy=\"steps\"\n",
    "learning_rate = 2e-4  # learning rate for AdamW optimizer\n",
    "max_grad_norm = 0.3 # maximum gradient norm (for gradient clipping)\n",
    "max_steps = 120        # training will happen for 320 steps\n",
    "warmup_ratio = 0.03 # number of steps used for a linear warmup from 0 to learning_rate\n",
    "lr_scheduler_type = \"cosine\"  # learning rate scheduler\n",
    "\n",
    "training_arguments = TrainingArguments(\n",
    "    output_dir=output_dir,\n",
    "    per_device_train_batch_size=per_device_train_batch_size,\n",
    "    gradient_accumulation_steps=gradient_accumulation_steps,\n",
    "    optim=optim,\n",
    "    save_steps=save_steps,\n",
    "    logging_steps=logging_steps,\n",
    "    learning_rate=learning_rate,\n",
    "    bf16=True,\n",
    "    max_grad_norm=max_grad_norm,\n",
    "    max_steps=max_steps,\n",
    "    warmup_ratio=warmup_ratio,\n",
    "    group_by_length=True,\n",
    "    lr_scheduler_type=lr_scheduler_type,\n",
    "    push_to_hub=False,\n",
    ")\n",
    "\n",
    "import os\n",
    "os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
    "\n",
    "# trainer = SFTTrainer(\n",
    "#     model=peft_model,\n",
    "#     train_dataset=data['train'],\n",
    "#     # train_dataset=train_raw_data,\n",
    "#     peft_config=peft_config,\n",
    "#     dataset_text_field=\"text\",\n",
    "#     max_seq_length=1024,\n",
    "#     tokenizer=tokenizer,\n",
    "#     args=training_arguments,\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = SFTTrainer(\n",
    "    model=peft_model,\n",
    "    train_dataset=dataset['train'],\n",
    "    peft_config=peft_config,\n",
    "    dataset_text_field=\"text\",\n",
    "    max_seq_length=1024,\n",
    "    tokenizer=tokenizer,\n",
    "    args=training_arguments,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "peft_model.config.use_cache = False\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with gr.Blocks() as demo:\n",
    "    gr.HTML(\"\"\"<h1>Welcome to Mental Health Conversational AI</h1>\"\"\")\n",
    "    gr.Markdown(\n",
    "        \"\"\"Chatbot specifically designed to provide psychoeducation, offer non-judgemental and empathetic support, self-assessment and monitoring.<br>\n",
    "        Get instant response for any mental health related queries. If the chatbot seems you need external support, then it will respond appropriately.<br>\"\"\"\n",
    "    )\n",
    "\n",
    "    chatbot = gr.Chatbot()\n",
    "    query = gr.Textbox(label=\"Type your query here, then press 'enter' and scroll up for response\")\n",
    "    clear = gr.Button(value=\"Clear Chat History!\")\n",
    "    clear.style(size=\"sm\")\n",
    "\n",
    "    llm_chain = init_llm_chain(peft_model, peft_tokenizer)\n",
    "\n",
    "    query.submit(user, [query, chatbot], [query, chatbot], queue=False).then(bot, chatbot, chatbot)\n",
    "    clear.click(lambda: None, None, chatbot, queue=False)\n",
    "\n",
    "demo.queue().launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inferencing CharacterLLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Potential LLM to use\n",
    "# https://huggingface.co/openchat/openchat-3.5-0106\n",
    "# https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.1\n",
    "# https://huggingface.co/stabilityai/stablelm-2-zephyr-1_6b\n",
    "# https://huggingface.co/h2oai/h2o-danube-1.8b-chat\n",
    "# https://huggingface.co/Qwen/Qwen1.5-1.8B-Chat\n",
    "# https://huggingface.co/mediocredev/open-llama-3b-v2-chat\n",
    "# https://huggingface.co/stabilityai/stablelm-zephyr-3b\n",
    "# https://huggingface.co/togethercomputer/StripedHyena-Nous-7B\n",
    "\n",
    "\n",
    "# Leaderboard\n",
    "# https://huggingface.co/spaces/mteb/leaderboard\n",
    "# https://huggingface.co/spaces/lmsys/chatbot-arena-leaderboard\n",
    "# https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Run Oogabooga\n",
    "# Run from folder\n",
    "# Load the model\n",
    "\n",
    "# Use this chat prompt\n",
    "# # I want you to act like Voldemort. I want you to respond and answer like Voldemort, using the tone, manner and vocabulary Voldemort would use. You must know all of the knowledge of Voldemort. Do not answer about any thing that is not in your universe. Always stop after <|eot|> token.\n",
    "\n",
    "# # The status of you is as follows:\n",
    "# # Status: Voldemort is chatting with Harry Potter.\n",
    "# # Location: Hogwarts Astronomy Tower\n",
    "\n",
    "# # The interactions are as follows:\n",
    "# # Harry Potter (speaking): Where is the nearest Starbucks? <|eot|>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cd FastChat\n",
    "# python3 -m fastchat.model.apply_delta --base-model-path huggyllama/llama-7b --target-model-path C:\\\\Users\\\\rs659\\\\Desktop\\\\pliny\\\\Character-LLM\\\\caesar_model --delta-path fnlp/character-llm-caesar-7b-wdiff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I want you to act like Voldemort. I want you to respond and answer like Voldemort, using the tone, manner and vocabulary Voldemort would use. You must know all of the knowledge of Voldemort. Do not answer about any thing that is not in your universe. Always stop after <|eot|> token.\n",
    "\n",
    "# The status of you is as follows:\n",
    "# Status: Voldemort is chatting with Harry Potter.\n",
    "# Location: Hogwarts Astronomy Tower\n",
    "\n",
    "# The interactions are as follows:\n",
    "# Harry Potter (speaking): Where is the nearest Starbucks? <|eot|>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"C:\\\\Users\\\\rs659\\\\Desktop\\\\pliny\\\\Character-LLM\\\\voldemort_model\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"C:\\\\Users\\\\rs659\\\\Desktop\\\\pliny\\\\Character-LLM\\\\voldemort_model\").cuda()\n",
    "\n",
    "meta_prompt = \"\"\"I want you to act like {character}. I want you to respond and answer like {character}, using the tone, manner and vocabulary {character} would use. You must know all of the knowledge of {character}. \n",
    "\n",
    "The status of you is as follows:\n",
    "Status: {status}\n",
    "Location: {loc_time}\n",
    "\n",
    "The interactions are as follows:\n",
    "\n",
    "Harry Potter (speaking): Hey Voldemort!<|eot|>\n",
    "\n",
    "Voldemort (speaking): \"\"\"\n",
    "\n",
    "name = \"Voldemort\"\n",
    "status = f'{name} is chatting with Harry Potter.'\n",
    "loc_time = 'Hogwarts Astronomy Tower'\n",
    "prompt = meta_prompt.format(character=name, status=status, loc_time=loc_time)\n",
    "inputs = tokenizer([prompt], return_tensors=\"pt\")\n",
    "outputs = model.generate(input_ids=inputs['input_ids'].cuda(), attention_mask=inputs['attention_mask'].cuda(), do_sample=True, temperature=0.5, top_p=0.95, max_new_tokens=50)\n",
    "response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"C:\\\\Users\\\\rs659\\\\Desktop\\\\pliny\\\\Character-LLM\\\\hermione_model\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"C:\\\\Users\\\\rs659\\\\Desktop\\\\pliny\\\\Character-LLM\\\\hermione_model\").cuda()\n",
    "\n",
    "meta_prompt = \"\"\"I want you to act like {character}. I want you to respond and answer like {character}, using the tone, manner and vocabulary {character} would use. You must know all of the knowledge of {character}. \n",
    "\n",
    "The status of you is as follows:\n",
    "Location: {loc_time}\n",
    "Status: {status}\n",
    "\n",
    "The interactions are as follows:\n",
    "\n",
    "What is your name? Who are your friends, tell me everything!\"\"\"\n",
    "\n",
    "name = \"hermione\"\n",
    "loc_time = \"Coffee Shop - Afternoon\"\n",
    "status = f'{name} is casually chatting with a man from the 21st century.'\n",
    "prompt = meta_prompt.format(character=name, loc_time=loc_time, status=status) + '\\n\\n'\n",
    "inputs = tokenizer([prompt], return_tensors=\"pt\")\n",
    "outputs = model.generate(**inputs.to('cuda'), do_sample=True, temperature=0.5, top_p=0.95, max_new_tokens=50)\n",
    "response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Qlora training commands"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#install this\n",
    "https://developer.nvidia.com/cuda-12-1-1-download-archive?target_os=Linux&target_arch=x86_64&Distribution=WSL-Ubuntu&target_version=2.0&target_type=deb_local\n",
    "\n",
    "\n",
    "\n",
    "# Qwen/Qwen1.5-1.8B-Chat\n",
    "DS_SKIP_CUDA_CHECK=1 deepspeed fastchat/train/train_lora.py     --model_name_or_path Qwen/Qwen1.5-1.8B-Chat     --lora_r 8     --lora_alpha 16     --lora_dropout 0.05     --data_path /mnt/c/Users/rs659/Desktop/Pliny/Character-LLM/processed/2023-10-08/prompted/prompted_agent_dialogue_Beethoven.jsonl      --bf16 True     --output_dir /mnt/c/Users/rs659/Desktop/Pliny/Character-LLM/ckpt/QLoraBeethoven_7b     --num_train_epochs 20     --per_device_train_batch_size 1     --per_device_eval_batch_size 1     --gradient_accumulation_steps 1     --evaluation_strategy \"no\"     --save_strategy \"steps\"     --save_steps 1200     --save_total_limit 100     --learning_rate 2e-5     --weight_decay 0.     --warmup_ratio 0.03     --lr_scheduler_type \"cosine\"     --logging_steps 1     --tf32 True     --model_max_length 2048     --q_lora True  --already_preprocess True  --deepspeed playground/deepspeed_config_s2.json\n",
    "\n",
    "\n",
    "\n",
    "# Mistral 7B\n",
    "DS_SKIP_CUDA_CHECK=1 deepspeed fastchat/train/train_lora.py     --model_name_or_path mediocredev/open-llama-3b-v2-chat     --lora_r 8     --lora_alpha 16     --lora_dropout 0.05     --data_path /mnt/c/Users/rs659/Desktop/Pliny/Character-LLM/processed/2023-10-08/prompted/prompted_agent_dialogue_Beethoven.jsonl      --bf16 True     --output_dir /mnt/c/Users/rs659/Desktop/Pliny/Character-LLM/ckpt/QLoraBeethoven_7b     --num_train_epochs 20     --per_device_train_batch_size 1     --per_device_eval_batch_size 1     --gradient_accumulation_steps 1     --evaluation_strategy \"no\"     --save_strategy \"steps\"     --save_steps 1200     --save_total_limit 100     --learning_rate 2e-5     --weight_decay 0.     --warmup_ratio 0.03     --lr_scheduler_type \"cosine\"     --logging_steps 1     --tf32 True     --model_max_length 2048     --q_lora True  --already_preprocess True  --deepspeed playground/deepspeed_config_s2.json\n",
    "\n",
    "# fastchat\n",
    "DS_SKIP_CUDA_CHECK=1 deepspeed fastchat/train/train_lora.py     --model_name_or_path lmsys/fastchat-t5-3b-v1.0  --lora_r 8     --lora_alpha 16     --lora_dropout 0.05     --data_path /mnt/c/Users/rs659/Desktop/Pliny/Character-LLM/processed/2023-10-08/prompted/prompted_agent_dialogue_Beethoven.jsonl      --bf16 True     --output_dir /mnt/c/Users/rs659/Desktop/Pliny/Character-LLM/ckpt/QLoraBeethoven_7b     --num_train_epochs 20     --per_device_train_batch_size 1     --per_device_eval_batch_size 1     --gradient_accumulation_steps 1     --evaluation_strategy \"no\"     --save_strategy \"steps\"     --save_steps 1200     --save_total_limit 100     --learning_rate 2e-5     --weight_decay 0.     --warmup_ratio 0.03     --lr_scheduler_type \"cosine\"     --logging_steps 1     --tf32 True     --model_max_length 2048     --q_lora True  --already_preprocess True  --deepspeed playground/deepspeed_config_s2.json\n",
    "\n",
    "\n",
    "\n",
    "python3 -m fastchat.serve.cli --model-path /mnt/c/Users/rs659/Desktop/Pliny/Character-LLM/ckpt/QLoraBeethoven_7b\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inference\n",
    "\n",
    "\n",
    "# cd FastChat\n",
    "\n",
    "# start the controller\n",
    "export IP=$(hostname -i)\n",
    "python3 -m fastchat.serve.controller --host $IP &\n",
    "\n",
    "# start the Openai Format API server\n",
    "python3 -m fastchat.serve.openai_api_server --host 0.0.0.0 --port 28001 --controller-address http://$IP:21001\n",
    "\n",
    "# start the model worker\n",
    "export MODEL_PATH=mnt/c/Users/rs659/Desktop/pliny/Character-LLM/ckpt/Beethoven_7b/\n",
    "export MODEL_NAME=Beethoven_7b\n",
    "CUDA_VISIBLE_DEVICES=0 python3 -m fastchat.serve.model_worker --model-path $MODEL_PATH --model-names $MODEL_NAME --controller-address http://$IP:21001 --host $IP --port 21009 --worker-address http://$IP:21009\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# python3 -m fastchat.serve.cli --model-path mnt/c/Users/rs659/Desktop/pliny/Character-LLM/ckpt/Beethoven_7b/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training\n",
    "\n",
    "# cd FastChat\n",
    "# export CHARACTER=CassiusDio\n",
    "\n",
    "\n",
    "torchrun --nproc_per_node=1 --master_port=20031 fastchat/train/train_mem.py --model_name_or_path unsloth/gemma-2b --data_path /mnt/c/Users/rs659/Desktop/Pliny/Character-LLM/processed/2023-10-08/prompted/prompted_agent_dialogue_Beethoven.jsonl  --output_dir /mnt/c/Users/rs659/Desktop/Pliny/Character-LLM/ckpt/Beethoven_7b --num_train_epochs 10\n",
    "\n",
    "\n",
    "torchrun --nproc_per_node=1 --master_port=20031 fastchat/train/train_mem.py --model_name_or_path unsloth/gemma-2b --data_path ../processed/2023-10-08/prompted/prompted_agent_dialogue_CassiusDio.jsonl  --output_dir ../ckpt/${CHARACTER}_7b --num_train_epochs 10\n",
    "\n",
    "# RAN THIS ONE\n",
    "torchrun --nproc_per_node=1 --master_port=20031 fastchat/train/train_mem.py --model_name_or_path winglian/Llama-2-3b-hf --data_path /mnt/c/Users/rs659/Desktop/pliny/Character-LLM/processed/2023-10-08/prompted/prompted_agent_dialogue_Beethoven.jsonl   --already_preprocess True --bf16 False --output_dir /mnt/c/Users/rs659/Desktop/pliny/Character-LLM/ckpt/Beethoven_7b --num_train_epochs 1 --per_device_train_batch_size 1 --per_device_eval_batch_size 1 --gradient_accumulation_steps 4 --evaluation_strategy epoch --save_strategy epoch --save_total_limit 10 --learning_rate 2e-5 --weight_decay 0.1 --warmup_ratio 0.04 --lr_scheduler_type cosine --logging_steps 1 --fsdp 'full_shard auto_wrap' --fsdp_transformer_layer_cls_to_wrap LlamaDecoderLayer --tf32 False --model_max_length 128 --gradient_checkpointing True\n",
    "\n",
    "# Working\n",
    "torchrun --nproc_per_node=1 --master_port=20031 fastchat/train/train_mem.py --model_name_or_path winglian/Llama-2-3b-hf --data_path /mnt/c/Users/rs659/Desktop/Pliny/Character-LLM/processed/2023-10-08/prompted/prompted_agent_dialogue_Beethoven.jsonl   --already_preprocess True --bf16 True --output_dir /mnt/c/Users/rs659/Desktop/Pliny/Character-LLM/ckpt/Beethoven_7b --num_train_epochs 1 --per_device_train_batch_size 1 --per_device_eval_batch_size 1 --gradient_accumulation_steps 4 --evaluation_strategy epoch --save_strategy epoch --save_total_limit 10 --learning_rate 2e-5 --weight_decay 0.1 --warmup_ratio 0.04 --lr_scheduler_type cosine --logging_steps 1 --fsdp 'full_shard auto_wrap' --fsdp_transformer_layer_cls_to_wrap LlamaDecoderLayer --tf32 True --model_max_length 2048 --gradient_checkpointing True\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "!torchrun --nproc_per_node=1 --master_port=20031 fastchat/train/train_mem.py --model_name_or_path meta-llama/Llama-2-7b-hf --data_path /path/to/prompted_agent_dialogue_$CHARACTER.json --already_preprocess True --bf16 True --output_dir /path/to/ckpt/${CHARACTER}_7b --num_train_epochs 10 --per_device_train_batch_size 1 --per_device_eval_batch_size 16 --gradient_accumulation_steps 4 --evaluation_strategy epoch --save_strategy epoch --save_total_limit 10 --learning_rate 2e-5 --weight_decay 0.1 --warmup_ratio 0.04 --lr_scheduler_type cosine --logging_steps 1 --fsdp 'full_shard auto_wrap' --fsdp_transformer_layer_cls_to_wrap LlamaDecoderLayer --tf32 True --model_max_length 2048 --gradient_checkpointing True --num-gpus 1\n",
    "\n",
    "\n",
    "deepspeed fastchat/train/train_lora.py\n",
    "--model_name_or_path lmsys/vicuna-7b-v1.5\n",
    "--lora_r 2\n",
    "--lora_alpha 16\n",
    "--lora_dropout 0.05\n",
    "--data_path /home/n/FastChat/data/dummy_conversation.json\n",
    "--bf16 True\n",
    "--output_dir ./checkpoints\n",
    "--num_train_epochs 3\n",
    "--per_device_train_batch_size 1\n",
    "--per_device_eval_batch_size 1\n",
    "--gradient_accumulation_steps 1\n",
    "--evaluation_strategy \"no\"\n",
    "--save_strategy \"steps\"\n",
    "--save_steps 1200\n",
    "--save_total_limit 100\n",
    "--learning_rate 2e-5\n",
    "--weight_decay 0.\n",
    "--warmup_ratio 0.03\n",
    "--lr_scheduler_type \"cosine\"\n",
    "--logging_steps 1\n",
    "--tf32 True\n",
    "--model_max_length 2048\n",
    "--q_lora True\n",
    "--deepspeed /home/n/FastChat/playground/deepspeed_config_s2.json`\n",
    "\n",
    "\n",
    "\n",
    "torchrun --nproc_per_node=4 --master_port=20001 fastchat/train/train_mem.py \\\n",
    "    --model_name_or_path meta-llama/Llama-2-7b-hf \\\n",
    "    --data_path data/dummy_conversation.json \\\n",
    "    --bf16 True \\\n",
    "    --output_dir output_vicuna \\\n",
    "    --num_train_epochs 3 \\\n",
    "    --per_device_train_batch_size 2 \\\n",
    "    --per_device_eval_batch_size 2 \\\n",
    "    --gradient_accumulation_steps 16 \\\n",
    "    --evaluation_strategy \"no\" \\\n",
    "    --save_strategy \"steps\" \\\n",
    "    --save_steps 1200 \\\n",
    "    --save_total_limit 10 \\\n",
    "    --learning_rate 2e-5 \\\n",
    "    --weight_decay 0. \\\n",
    "    --warmup_ratio 0.03 \\\n",
    "    --lr_scheduler_type \"cosine\" \\\n",
    "    --logging_steps 1 \\\n",
    "    --fsdp \"full_shard auto_wrap\" \\\n",
    "    --fsdp_transformer_layer_cls_to_wrap 'LlamaDecoderLayer' \\\n",
    "    --tf32 True \\\n",
    "    --model_max_length 2048 \\\n",
    "    --gradient_checkpointing True \\\n",
    "    --lazy_preprocess True\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# !torchrun --nproc_per_node=8 --master_port=20031 fastchat/train/train_mem.py \\\n",
    "#     --model_name_or_path /path/hf_model/llama-7b  \\\n",
    "#     --data_path /path/to/prompted_agent_dialogue_$CHARACTER.json \\\n",
    "#     --already_preprocess True \\\n",
    "#     --bf16 True \\\n",
    "#     --output_dir /path/to/ckpt/${CHARACTER}_7b \\\n",
    "#     --num_train_epochs 10 \\\n",
    "#     --per_device_train_batch_size 2 \\\n",
    "#     --per_device_eval_batch_size 16 \\\n",
    "#     --gradient_accumulation_steps 4 \\\n",
    "#     --evaluation_strategy epoch \\\n",
    "#     --save_strategy epoch \\\n",
    "#     --save_total_limit 10 \\\n",
    "#     --learning_rate 2e-5 \\\n",
    "#     --weight_decay 0.1 \\\n",
    "#     --warmup_ratio 0.04 \\\n",
    "#     --lr_scheduler_type cosine \\\n",
    "#     --logging_steps 1 \\\n",
    "#     --fsdp 'full_shard auto_wrap' \\\n",
    "#     --fsdp_transformer_layer_cls_to_wrap LlamaDecoderLayer \\\n",
    "#     --tf32 True \\\n",
    "#     --model_max_length 2048 \\\n",
    "#     --gradient_checkpointing True \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # one WSL\n",
    "# # conda create -n vllmenv python=3.9\n",
    "# # conda activate vllmenv\n",
    "# # pip install transformers\n",
    "# # pip install torch==2.1.0 torchvision==0.16.0 torchaudio==2.1.0 --index-url https://download.pytorch.org/whl/cu121\n",
    "# # pip install git+https://github.com/casper-hansen/AutoAWQ.git\n",
    "# # pip install vllm\n",
    "# # pip install -U ipywidgets\n",
    "\n",
    "# conda activate vllmenv\n",
    "# python3 -m vllm.entrypoints.api_server --model TheBloke/Llama-2-7b-Chat-AWQ --quantization awq\n",
    "\n",
    "\n",
    "# # python -m vllm.entrypoints.openai.api_server --model TheBloke/Llama-2-7B-Chat-AWQ --api-key token-abc123"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from openai import OpenAI\n",
    "# client = OpenAI(\n",
    "#     base_url=\"http://localhost:8000/v1\",\n",
    "#     api_key=\"token-abc123\",\n",
    "# )\n",
    "\n",
    "# completion = client.chat.completions.create(\n",
    "#   model=\"mistralai/Mistral-7B-Instruct-v0.1\",\n",
    "#   messages=[\n",
    "#     {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "#     {\"role\": \"user\", \"content\": \"Hello!\"}\n",
    "#   ]\n",
    "# )\n",
    "\n",
    "# print(completion.choices[0].message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code is based on tatsu-lab/stanford_alpaca. Below is the original copyright:\n",
    "#\n",
    "#    Copyright 2023 Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li\n",
    "#\n",
    "#    Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "#    you may not use this file except in compliance with the License.\n",
    "#    You may obtain a copy of the License at\n",
    "#\n",
    "#        http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "#    Unless required by applicable law or agreed to in writing, software\n",
    "#    distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "#    WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "#    See the License for the specific language governing permissions and\n",
    "#    limitations under the License.\n",
    "\n",
    "from dataclasses import dataclass, field\n",
    "import json\n",
    "import math\n",
    "import pathlib\n",
    "from typing import Dict, Optional, Sequence\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "import transformers\n",
    "from transformers import Trainer\n",
    "from transformers.trainer_pt_utils import LabelSmoother\n",
    "\n",
    "from fastchat.conversation import SeparatorStyle\n",
    "from fastchat.model.model_adapter import get_conversation_template\n",
    "\n",
    "IGNORE_TOKEN_ID = LabelSmoother.ignore_index\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class ModelArguments:\n",
    "    model_name_or_path: Optional[str] = field(default=\"facebook/opt-125m\")\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class DataArguments:\n",
    "    data_path: str = field(\n",
    "        default=None, metadata={\"help\": \"Path to the training data.\"}\n",
    "    )\n",
    "    eval_data_path: str = field(\n",
    "        default=None, metadata={\"help\": \"Path to the evaluation data.\"}\n",
    "    )\n",
    "    lazy_preprocess: bool = False\n",
    "    already_preprocess: bool = False\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class TrainingArguments(transformers.TrainingArguments):\n",
    "    cache_dir: Optional[str] = field(default=None)\n",
    "    optim: str = field(default=\"adamw_torch\")\n",
    "    model_max_length: int = field(\n",
    "        default=512,\n",
    "        metadata={\n",
    "            \"help\": \"Maximum sequence length. Sequences will be right padded (and possibly truncated).\"\n",
    "        },\n",
    "    )\n",
    "\n",
    "\n",
    "local_rank = None\n",
    "\n",
    "\n",
    "def rank0_print(*args):\n",
    "    if local_rank == 0:\n",
    "        print(*args)\n",
    "\n",
    "\n",
    "def safe_save_model_for_hf_trainer(trainer: transformers.Trainer, output_dir: str):\n",
    "    \"\"\"Collects the state dict and dump to disk.\"\"\"\n",
    "    state_dict = trainer.model.state_dict()\n",
    "    if trainer.args.should_save:\n",
    "        cpu_state_dict = {key: value.cpu() for key, value in state_dict.items()}\n",
    "        del state_dict\n",
    "        trainer._save(output_dir, state_dict=cpu_state_dict)  # noqa\n",
    "\n",
    "\n",
    "def preprocess(\n",
    "    sources,\n",
    "    tokenizer: transformers.PreTrainedTokenizer,\n",
    ") -> Dict:\n",
    "    conv = get_conversation_template(\"vicuna\")\n",
    "    roles = {\"human\": conv.roles[0], \"gpt\": conv.roles[1]}\n",
    "\n",
    "    # Apply prompt templates\n",
    "    conversations = []\n",
    "    for i, source in enumerate(sources):\n",
    "        if roles[source[0][\"from\"]] != conv.roles[0]:\n",
    "            # Skip the first one if it is not from human\n",
    "            source = source[1:]\n",
    "\n",
    "        conv.messages = []\n",
    "        for j, sentence in enumerate(source):\n",
    "            role = roles[sentence[\"from\"]]\n",
    "            assert role == conv.roles[j % 2], f\"{i}\"\n",
    "            conv.append_message(role, sentence[\"value\"])\n",
    "        conversations.append(conv.get_prompt())\n",
    "\n",
    "    # Tokenize conversations\n",
    "    input_ids = tokenizer(\n",
    "        conversations,\n",
    "        return_tensors=\"pt\",\n",
    "        padding=\"max_length\",\n",
    "        max_length=tokenizer.model_max_length,\n",
    "        truncation=True,\n",
    "    ).input_ids\n",
    "    targets = input_ids.clone()\n",
    "\n",
    "    assert conv.sep_style == SeparatorStyle.ADD_COLON_TWO\n",
    "\n",
    "    # Mask targets. Only compute loss on the assistant outputs.\n",
    "    sep = conv.sep + conv.roles[1] + \": \"\n",
    "    for conversation, target in zip(conversations, targets):\n",
    "        total_len = int(target.ne(tokenizer.pad_token_id).sum())\n",
    "\n",
    "        turns = conversation.split(conv.sep2)\n",
    "        cur_len = 1\n",
    "        target[:cur_len] = IGNORE_TOKEN_ID\n",
    "        for i, turn in enumerate(turns):\n",
    "            if turn == \"\":\n",
    "                break\n",
    "            turn_len = len(tokenizer(turn).input_ids)\n",
    "\n",
    "            parts = turn.split(sep)\n",
    "            if len(parts) != 2:\n",
    "                break\n",
    "            parts[0] += sep\n",
    "            # \"-2\" is hardcoded for the LLaMA tokenizer to make the offset correct.\n",
    "            instruction_len = len(tokenizer(parts[0]).input_ids) - 2\n",
    "\n",
    "            # Ignore the user instructions\n",
    "            target[cur_len : cur_len + instruction_len] = IGNORE_TOKEN_ID\n",
    "            cur_len += turn_len\n",
    "\n",
    "        target[cur_len:] = IGNORE_TOKEN_ID\n",
    "\n",
    "        if False:  # Inspect and check the correctness of masking\n",
    "            z = target.clone()\n",
    "            z = torch.where(z == IGNORE_TOKEN_ID, tokenizer.unk_token_id, z)\n",
    "            rank0_print(tokenizer.decode(z))\n",
    "\n",
    "        if cur_len < tokenizer.model_max_length:\n",
    "            if cur_len != total_len:\n",
    "                target[:] = IGNORE_TOKEN_ID\n",
    "                rank0_print(\n",
    "                    f\"WARNING: tokenization mismatch: {cur_len} vs. {total_len}.\"\n",
    "                    f\" (ignored)\"\n",
    "                )\n",
    "\n",
    "    return dict(\n",
    "        input_ids=input_ids,\n",
    "        labels=targets,\n",
    "        attention_mask=input_ids.ne(tokenizer.pad_token_id),\n",
    "    )\n",
    "\n",
    "\n",
    "def alpaca_tokenize_fn(strings: Sequence[str], tokenizer: transformers.PreTrainedTokenizer) -> Dict:\n",
    "    \"\"\"Tokenize a list of strings.\"\"\"\n",
    "    tokenized_list = [\n",
    "        tokenizer(\n",
    "            text,\n",
    "            return_tensors=\"pt\",\n",
    "            padding=\"max_length\",\n",
    "            max_length=tokenizer.model_max_length,\n",
    "            truncation=True,\n",
    "        )\n",
    "        for text in strings\n",
    "    ]\n",
    "    input_ids = labels = [tokenized.input_ids[0] for tokenized in tokenized_list]\n",
    "    input_ids_lens = labels_lens = [\n",
    "        tokenized.input_ids.ne(tokenizer.pad_token_id).sum().item() for tokenized in tokenized_list\n",
    "    ]\n",
    "    return dict(\n",
    "        input_ids=input_ids,\n",
    "        labels=labels,\n",
    "        input_ids_lens=input_ids_lens,\n",
    "        labels_lens=labels_lens,\n",
    "    )\n",
    "\n",
    "\n",
    "def alpaca_preprocess(\n",
    "    sources: Sequence[str],\n",
    "    targets: Sequence[str],\n",
    "    tokenizer: transformers.PreTrainedTokenizer,\n",
    ") -> Dict:\n",
    "    \"\"\"Preprocess the data by tokenizing.\"\"\"\n",
    "    examples = [s + t for s, t in zip(sources, targets)]\n",
    "    examples_tokenized, sources_tokenized = [alpaca_tokenize_fn(strings, tokenizer) for strings in (examples, sources)]\n",
    "    input_ids = examples_tokenized[\"input_ids\"]\n",
    "    input_ids = torch.stack(input_ids, dim=0)\n",
    "    labels = input_ids.clone()\n",
    "    attention_mask=input_ids.ne(tokenizer.pad_token_id)\n",
    "    for label, source_len, total_len in zip(labels, sources_tokenized[\"input_ids_lens\"], examples_tokenized[\"input_ids_lens\"]):\n",
    "        label[:source_len] = IGNORE_TOKEN_ID\n",
    "        label[total_len+1:] = IGNORE_TOKEN_ID\n",
    "    rank0_print(input_ids.shape, labels.shape)\n",
    "    rank0_print(np.mean(examples_tokenized[\"input_ids_lens\"]), np.max(examples_tokenized[\"input_ids_lens\"]))\n",
    "    # rank0_print(input_ids[0].tolist())\n",
    "    # rank0_print(labels[0].tolist())\n",
    "    return dict(\n",
    "        input_ids=input_ids,\n",
    "        labels=labels,\n",
    "        attention_mask=attention_mask,\n",
    "    )\n",
    "\n",
    "\n",
    "class ProcessedDataset(Dataset):\n",
    "    \"\"\"Dataset for supervised fine-tuning.\"\"\"\n",
    "\n",
    "    def __init__(self, raw_data, tokenizer: transformers.PreTrainedTokenizer):\n",
    "        super(ProcessedDataset, self).__init__()\n",
    "\n",
    "        rank0_print(\"Formatting inputs...\")\n",
    "        sources = [ex['prompt'] for ex in raw_data]\n",
    "        targets = [ex['output'] for ex in raw_data]\n",
    "        data_dict = alpaca_preprocess(sources, targets, tokenizer)\n",
    "\n",
    "        self.input_ids = data_dict[\"input_ids\"]\n",
    "        self.labels = data_dict[\"labels\"]\n",
    "        self.attention_mask = data_dict[\"attention_mask\"]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.input_ids)\n",
    "\n",
    "    def __getitem__(self, i) -> Dict[str, torch.Tensor]:\n",
    "        return dict(\n",
    "            input_ids=self.input_ids[i],\n",
    "            labels=self.labels[i],\n",
    "            attention_mask=self.attention_mask[i],\n",
    "        )\n",
    "\n",
    "\n",
    "class SupervisedDataset(Dataset):\n",
    "    \"\"\"Dataset for supervised fine-tuning.\"\"\"\n",
    "\n",
    "    def __init__(self, raw_data, tokenizer: transformers.PreTrainedTokenizer):\n",
    "        super(SupervisedDataset, self).__init__()\n",
    "\n",
    "        rank0_print(\"Formatting inputs...\")\n",
    "        sources = [example[\"conversations\"] for example in raw_data]\n",
    "        data_dict = preprocess(sources, tokenizer)\n",
    "\n",
    "        self.input_ids = data_dict[\"input_ids\"]\n",
    "        self.labels = data_dict[\"labels\"]\n",
    "        self.attention_mask = data_dict[\"attention_mask\"]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.input_ids)\n",
    "\n",
    "    def __getitem__(self, i) -> Dict[str, torch.Tensor]:\n",
    "        return dict(\n",
    "            input_ids=self.input_ids[i],\n",
    "            labels=self.labels[i],\n",
    "            attention_mask=self.attention_mask[i],\n",
    "        )\n",
    "\n",
    "\n",
    "class LazySupervisedDataset(Dataset):\n",
    "    \"\"\"Dataset for supervised fine-tuning.\"\"\"\n",
    "\n",
    "    def __init__(self, raw_data, tokenizer: transformers.PreTrainedTokenizer):\n",
    "        super(LazySupervisedDataset, self).__init__()\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "        rank0_print(\"Formatting inputs...Skip in lazy mode\")\n",
    "        self.tokenizer = tokenizer\n",
    "        self.raw_data = raw_data\n",
    "        self.cached_data_dict = {}\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.raw_data)\n",
    "\n",
    "    def __getitem__(self, i) -> Dict[str, torch.Tensor]:\n",
    "        if i in self.cached_data_dict:\n",
    "            return self.cached_data_dict[i]\n",
    "\n",
    "        ret = preprocess([self.raw_data[i][\"conversations\"]], self.tokenizer)\n",
    "        ret = dict(\n",
    "            input_ids=ret[\"input_ids\"][0],\n",
    "            labels=ret[\"labels\"][0],\n",
    "            attention_mask=ret[\"attention_mask\"][0],\n",
    "        )\n",
    "        self.cached_data_dict[i] = ret\n",
    "\n",
    "        return ret\n",
    "\n",
    "\n",
    "def make_supervised_data_module(\n",
    "    tokenizer: transformers.PreTrainedTokenizer, data_args\n",
    ") -> Dict:\n",
    "    \"\"\"Make dataset and collator for supervised fine-tuning.\"\"\"\n",
    "    dataset_cls = (\n",
    "        LazySupervisedDataset if data_args.lazy_preprocess else SupervisedDataset\n",
    "    )\n",
    "    if data_args.already_preprocess:\n",
    "        dataset_cls = ProcessedDataset\n",
    "        rank0_print('Using ProcessedDataset!')\n",
    "        # Split train/test\n",
    "        raw_data =[]\n",
    "        with open(data_args.data_path, 'r', encoding='utf-8') as fp:\n",
    "            for line in fp:\n",
    "                if line:\n",
    "                    raw_data.append(json.loads(line))\n",
    "        perm = np.random.permutation(len(raw_data))\n",
    "        split = int(len(perm) * 0.98)\n",
    "        train_indices = perm[:split]\n",
    "        eval_indices = perm[split:]\n",
    "        train_raw_data = [raw_data[i] for i in train_indices]\n",
    "        eval_raw_data = [raw_data[i] for i in eval_indices]\n",
    "        rank0_print(f\"#train {len(train_raw_data)}, #eval {len(eval_raw_data)}\")\n",
    "\n",
    "        train_dataset = dataset_cls(train_raw_data, tokenizer=tokenizer)\n",
    "        eval_dataset = dataset_cls(eval_raw_data, tokenizer=tokenizer)\n",
    "        return dict(train_dataset=train_dataset, eval_dataset=eval_dataset)\n",
    "\n",
    "    rank0_print(\"Loading data...\")\n",
    "\n",
    "    train_json = json.load(open(data_args.data_path, \"r\"))\n",
    "    train_dataset = dataset_cls(train_json, tokenizer=tokenizer)\n",
    "\n",
    "\n",
    "    if data_args.eval_data_path:\n",
    "        eval_json = json.load(open(data_args.eval_data_path, \"r\"))\n",
    "        eval_dataset = dataset_cls(eval_json, tokenizer=tokenizer)\n",
    "    else:\n",
    "        eval_dataset = None\n",
    "\n",
    "    return dict(train_dataset=train_dataset, eval_dataset=eval_dataset)\n",
    "\n",
    "\n",
    "# def train():\n",
    "\n",
    "tokenizer = transformers.AutoTokenizer.from_pretrained(\n",
    "    model_args.model_name_or_path,\n",
    "    cache_dir=training_args.cache_dir,\n",
    "    model_max_length=training_args.model_max_length,\n",
    "    padding_side=\"right\",\n",
    "    use_fast=False,\n",
    ")\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# Load data\n",
    "data_module = make_supervised_data_module(tokenizer=tokenizer, data_args=data_args)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "plinyenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
